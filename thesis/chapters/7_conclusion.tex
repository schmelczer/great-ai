\chapter{Conclusion} \label{chapter:conclusion}

Concerned by the asymmetry between the industry's adoption of accessible AI/ML-libraries and existing solutions for their robust deployment, we investigated this phenomenon's causes and potential resolution. When looking at various recent case studies, a recurring theme was revealed: \textit{transitioning} from prototype to production-ready AI/ML deployment is a source of adversity for small and large enterprises alike. Even though several frameworks and platforms exist for facilitating this step, surveys on the execution of best practices continue to expose the industry's shortcomings. This signals that existing libraries are underutilised, which may lead to poor AI deployments that underperform or develop issues that go unnoticed and might inflict societal harm.

It was hypothesised that presenting a library which implements best practices and is also optimised for ease of adoption could help increase the overall quality of industrial AI/ML deployments. To test this, a framework was designed and implemented based on the principles of cognitive science and the prior art of software design. The design was subsequently tested and refined in an iterative process. First, a model was developed and deployed for classifying the domains of academic publications. Then, a SciBERT model was fine-tuned and deployed for generating the technology-transfer summaries of the same publications. \textit{GreatAI} had been proven helpful; therefore, after feeding back the insights gained into its design, it was turned into an open-source library. Furthermore, \textit{GreatAI} has been successfully integrated into every production deployment of ScoutinScience since then and receives thousands of monthly downloads.

During the refinement of the framework, six previously unaddressed AI/ML deployment best practices were identified. Including these, the framework fully implements 17 best practices while it provides support for another 16. The value provided by implementing or helping to implement these practices was validated through interviews with ten industry professionals from various subfields.

The interview participants completed two questionnaires, the results of one of which indicated that using \textit{GreatAI} in an example task increased the number of implemented best practices, on average, by 49\% compared with their latest project. The technology acceptance model was also calculated for the context; a significantly strong correlation was measured between the perceived ease of use, the perceived utility and the intention to use dimensions. Overall, proving that ease of use is just as important as core functionality when it comes to adopting AI deployment frameworks.

The open-ended exit interviews revealed that value can be derived from the library even in its current form and that the API's design has the opportunity to generalise to other fields of industrial AI/ML applications. However, they also highlighted that adoption issues do not necessarily come from a lack of willingness but a lack of awareness. Even if the returns achievable from good deployments are well worth the investment. Nevertheless, this value proposition needs to be conveyed and proved to data science professionals and technical decision-makers; and \textit{GreatAI} might just be the ideal candidate for that.

\textit{GreatAI} may have the potential to bridge the gap between data science and software engineering. Stemming from the bidirectional nature of bridges, we can look at the framework from two perspectives: for professionals closer to the field of data science, it provides an automatic scaffolding of software facilities that are required for deploying, monitoring, and iterating on their models. For software engineers, it highlights the necessary steps required for robust and improvable deployments. At the same time, it also saves them from the menial work of manually implementing these constructs. While most importantly, it proves that increasing the adoption rate of AI/ML deployment best practices is feasible by designing narrower and deeper APIs.

Good deployments benefit all of us. Accordingly, continued research into the means of good deployments remains crucial. However, next to that --- as the presented results have shown --- better deployments can also be achieved by facilitating the \textit{transition} step of the AI lifecycle with a focus on adoptability. Having automated implementations, even if for just the straightforward best practices, leaves professionals additional time to tackle the more complex deployment challenges and fewer opportunities to miss critical steps. Overall, resulting in more general, robust, end-to-end, automated, and trustworthy AI deployments.
