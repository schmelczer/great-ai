{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harden and deploy your app\n",
    "\n",
    "Finally, it's time to deploy your model. But before you have to make sure you follow AI deployment [best-practices](https://se-ml.github.io/). In the past, this step was too often either the source of unexpected struggles, or worse, simply ignored.\n",
    "\n",
    "With `GreatAI`, it has become a matter of 2 lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;226m2022-07-09 22:38:56 |  WARNING | Environment variable ENVIRONMENT is not set, defaulting to development mode ‼️\u001b[0m\n",
      "\u001b[38;5;226m2022-07-09 22:38:56 |  WARNING | Cannot find credentials files, defaulting to using ParallelTinyDbDriver\u001b[0m\n",
      "\u001b[38;5;226m2022-07-09 22:38:56 |  WARNING | The selected tracing database (ParallelTinyDbDriver) is not recommended for production\u001b[0m\n",
      "\u001b[38;5;226m2022-07-09 22:38:56 |  WARNING | Cannot find credentials files, defaulting to using LargeFileLocal\u001b[0m\n",
      "\u001b[38;5;39m2022-07-09 22:38:56 |     INFO | GreatAI (v0.1.0): configured ✅\u001b[0m\n",
      "\u001b[38;5;39m2022-07-09 22:38:56 |     INFO |   🔩 tracing_database: ParallelTinyDbDriver\u001b[0m\n",
      "\u001b[38;5;39m2022-07-09 22:38:56 |     INFO |   🔩 large_file_implementation: LargeFileLocal\u001b[0m\n",
      "\u001b[38;5;39m2022-07-09 22:38:56 |     INFO |   🔩 is_production: False\u001b[0m\n",
      "\u001b[38;5;39m2022-07-09 22:38:56 |     INFO |   🔩 should_log_exception_stack: True\u001b[0m\n",
      "\u001b[38;5;39m2022-07-09 22:38:56 |     INFO |   🔩 prediction_cache_size: 512\u001b[0m\n",
      "\u001b[38;5;39m2022-07-09 22:38:56 |     INFO |   🔩 dashboard_table_size: 20\u001b[0m\n",
      "\u001b[38;5;226m2022-07-09 22:38:56 |  WARNING | You still need to check whether you follow all best practices before trusting your deployment.\u001b[0m\n",
      "\u001b[38;5;226m2022-07-09 22:38:56 |  WARNING | > Find out more at https://se-ml.github.io/practices\u001b[0m\n",
      "\u001b[38;5;39m2022-07-09 22:38:56 |     INFO | Fetching cached versions of my-domain-predictor\u001b[0m\n",
      "\u001b[38;5;39m2022-07-09 22:38:56 |     INFO | Latest version of my-domain-predictor is 5 (from versions: 0, 1, 2, 3, 4, 5)\u001b[0m\n",
      "\u001b[38;5;39m2022-07-09 22:38:56 |     INFO | File my-domain-predictor-5 found in cache\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from great_ai import GreatAI, use_model\n",
    "from great_ai.utilities import clean\n",
    "\n",
    "\n",
    "@GreatAI.create\n",
    "@use_model(\"my-domain-predictor\")\n",
    "def predict_domain(sentence, model):\n",
    "    inputs = [clean(sentence)]\n",
    "    return str(model.predict(inputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trace[str]({ 'created': '2022-07-09T20:38:56.394746',\n",
       "  'exception': None,\n",
       "  'feedback': None,\n",
       "  'logged_values': { 'arg:sentence:length': 29,\n",
       "                     'arg:sentence:value': 'Mountains are just big rocks.'},\n",
       "  'models': [{'key': 'my-domain-predictor', 'version': 5}],\n",
       "  'original_execution_time_ms': 4.999,\n",
       "  'output': 'geography',\n",
       "  'tags': ['predict_domain', 'online', 'development'],\n",
       "  'trace_id': 'aad1f83d-a81f-4b8b-898e-d02f8076616f'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_domain(\"Mountains are just big rocks.\")\n",
    "# the original return value is under the 'output' key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;39m2022-07-09 22:38:58 |     INFO | Converting notebook to Python script\u001b[0m\n",
      "\u001b[38;5;39m2022-07-09 22:38:58 |     INFO | Found `predict_domain` to be the GreatAI app \u001b[0m\n",
      "\u001b[38;5;39m2022-07-09 22:38:58 |     INFO | Uvicorn running on http://0.0.0.0:6060 (Press CTRL+C to quit)\u001b[0m\n",
      "\u001b[38;5;226m2022-07-09 22:39:00 |  WARNING | Environment variable ENVIRONMENT is not set, defaulting to development mode ‼️\u001b[0m\n",
      "\u001b[38;5;226m2022-07-09 22:39:00 |  WARNING | Cannot find credentials files, defaulting to using ParallelTinyDbDriver\u001b[0m\n",
      "\u001b[38;5;226m2022-07-09 22:39:00 |  WARNING | The selected tracing database (ParallelTinyDbDriver) is not recommended for production\u001b[0m\n",
      "\u001b[38;5;226m2022-07-09 22:39:00 |  WARNING | Cannot find credentials files, defaulting to using LargeFileLocal\u001b[0m\n",
      "\u001b[38;5;39m2022-07-09 22:39:00 |     INFO | GreatAI (v0.1.0): configured ✅\u001b[0m\n",
      "\u001b[38;5;39m2022-07-09 22:39:00 |     INFO |   🔩 tracing_database: ParallelTinyDbDriver\u001b[0m\n",
      "\u001b[38;5;39m2022-07-09 22:39:00 |     INFO |   🔩 large_file_implementation: LargeFileLocal\u001b[0m\n",
      "\u001b[38;5;39m2022-07-09 22:39:00 |     INFO |   🔩 is_production: False\u001b[0m\n",
      "\u001b[38;5;39m2022-07-09 22:39:00 |     INFO |   🔩 should_log_exception_stack: True\u001b[0m\n",
      "\u001b[38;5;39m2022-07-09 22:39:00 |     INFO |   🔩 prediction_cache_size: 512\u001b[0m\n",
      "\u001b[38;5;39m2022-07-09 22:39:00 |     INFO |   🔩 dashboard_table_size: 20\u001b[0m\n",
      "\u001b[38;5;226m2022-07-09 22:39:00 |  WARNING | You still need to check whether you follow all best practices before trusting your deployment.\u001b[0m\n",
      "\u001b[38;5;226m2022-07-09 22:39:00 |  WARNING | > Find out more at https://se-ml.github.io/practices\u001b[0m\n",
      "\u001b[38;5;39m2022-07-09 22:39:00 |     INFO | Fetching cached versions of my-domain-predictor\u001b[0m\n",
      "\u001b[38;5;39m2022-07-09 22:39:00 |     INFO | Latest version of my-domain-predictor is 5 (from versions: 0, 1, 2, 3, 4, 5)\u001b[0m\n",
      "\u001b[38;5;39m2022-07-09 22:39:00 |     INFO | File my-domain-predictor-5 found in cache\u001b[0m\n",
      "\u001b[38;5;39m2022-07-09 22:39:00 |     INFO | Started server process [882179]\u001b[0m\n",
      "\u001b[38;5;39m2022-07-09 22:39:00 |     INFO | Waiting for application startup.\u001b[0m\n",
      "\u001b[38;5;39m2022-07-09 22:39:00 |     INFO | Application startup complete.\u001b[0m\n",
      "^C\n",
      "\u001b[38;5;39m2022-07-09 22:39:04 |     INFO | Shutting down\u001b[0m\n",
      "\u001b[38;5;39m2022-07-09 22:39:04 |     INFO | Waiting for application shutdown.\u001b[0m\n",
      "\u001b[38;5;39m2022-07-09 22:39:04 |     INFO | Application shutdown complete.\u001b[0m\n",
      "\u001b[38;5;39m2022-07-09 22:39:04 |     INFO | Finished server process [882179]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!great-ai deploy.ipynb\n",
    "# leave this running and open http://127.0.0.1:6060"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've made sure your application is hardened enough for the intended use case, it is time to deploy it. The responsibilities of GreatAI end when it wraps your inference function and model into a production-ready service. You're given the freedom and responsibility to deploy this service. Fortunately, you (or your organisation) probably already has an established routine for deploying services.\n",
    "\n",
    "There are three main approaches to deploy a GreatAI service.\n",
    "\n",
    "### Manual deployment\n",
    "\n",
    "Simply run `ENVIRONMENT=production great-ai deploy.ipynb` in the command-line of a production machine.\n",
    "> This is the crudest approach, however, it might be fitting for some contexts.\n",
    "\n",
    "### Containerised deployment\n",
    "\n",
    "Run the notebook directly in a container or create a service for it using your favourite orchestrator.\n",
    "\n",
    "```sh\n",
    "docker run \\\n",
    "    -p 6060:6060 \\\n",
    "    --volume `pwd`:/app \\\n",
    "    --rm \\\n",
    "    schmelczera/great-ai deploy.ipynb\n",
    "```\n",
    "> You can replace ``pwd`` with the path to your code's folder.\n",
    "\n",
    "#### Use a Platform-as-a-Service\n",
    "\n",
    "Similarly to the previous approach, your code will run in a container. However, instead of manually managing it, you can just choose from a plethora of PaaS providers that take a Docker image as a source and handle the rest of the deployment.\n",
    "\n",
    "To this end, you can also create a custom Docker image. It is especially useful if you have third-party dependencies, such as pytorch or tensorflow.\n",
    "\n",
    "```Dockerfile\n",
    "FROM schmelczera/great-ai:latest\n",
    "\n",
    "# Remove this block if you don't have a requirements.txt\n",
    "COPY requirements.txt ./   \n",
    "RUN pip install --no-cache-dir --requirement requirements.txt\n",
    "\n",
    "# If you store your models in S3 or GridFS, it may be a \n",
    "# good idea to # cache them in the image so that you don't\n",
    "# have to download it each time a container starts\n",
    "# RUN large-file --backend s3 --secrets s3.ini --cache my-domain-predictor\n",
    "\n",
    "COPY . .\n",
    "\n",
    "CMD [\"deploy.ipynb\"]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Go back to the summary](/tutorial)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02dd6d3afbfa9fbbe1037d64ad9014965528a1ccad21929d6e72f466389a68ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
